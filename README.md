# AdvancedAiGame01
# 주요 게임 설계

## 개발 배경 및 핵심 기능

마르코프 프로세스는 현재 상태 Si가 있을때, 다음 상태 Si+1을 예측하는 모델입니다

따라서 미래 예측이 중요한 게임을 만들어야 합니다.
또한 마르코프 속성에 따라 해당 게임은 아주 가까운 미래를 예측하는것을 목적으로 하고, 먼 과거의 정보들은 아무런 영향도 끼치지 말아야 합니다.

그러므로, 다음 기능을 예측할 수 있습니다
"서로 대립하는 효과를 가지면서, 먼저 사용한 AI가 유리한 전투 스킬을 만들자."
해당 스킬들은 무제한적으로 사용할 수 없고, 한번 사용하면 N턴동안 그 스킬(정책)을 사용할 수 없습니다. 또한 마르코프 속성에 따라, 다음 턴을 예측하는것이 중요한 주제이므로, 해당 스킬을 사용하고 나면, 다음 턴에 그 효과가 발휘되도록 제한을 걸어야 합니다.

첫번째 예시) 도약 스킬 vs 속박 스킬
- 도약 스킬은 순간이동을 하는 스킬입니다. 따라서 전투 발생 시 적이 도망치거나 혹은 임박한 적의 위험한 공격을 회피할때 사용할 수 있습니다.
- 속박 스킬은 상대방의 이동을 잠깐동안 제한하는 스킬입니다. 따라서 적이 도망치고 있을 때 해당 스킬을 사용하여 공격을 확실하게 넣을 수 있습니다.
- A와 B가 대립을 하고 있을때, A가 먼저 속박스킬을 사용하고 B가 나중에 도약 스킬을 사용한 경우, B는 속박 상태가 되어 도약 스킬이 무효화됩니다.
- A와 B가 대립을 하고 있을때, B가 먼저 도약 스킬을 사용하고 A가 나중에 속박 스킬을 사용한 경우, A의 속박 공격이 빗나가면서 속박 스킬이 무효화됩니다.
두번째 예시) 누크 스킬 vs 방어 스킬
- 누크는 핵으로 공격한다 라는 동사 nuke에서 파생된 게임 용어로, 짧은 시간동안 강한 공격을 집중시키는 행위를 의미합니다. 따라서 해당 누크 스킬은 상대방을 확실히 제거시키기 위한 능력입니다.
- 방어 스킬은 적의 공격을 막아내는 스킬인데, 상대방의 공격을 무효화시킵니다.
- A와 B가 대립을 하고 있을때, A가 먼저 누크 스킬을 사용하고 B가 나중에 방어 스킬을 사용한 경우, B는 공격을 받게 됩니다.
- A와 B가 대립을 하고 있을때, B가 먼저 이동 스킬을 사용하고 A가 나중에 속박 스킬을 사용한 경우, B의 방어 스킬이 먼저 작동되면서 A의 누크 스킬이 무효화됩니다.

## 상태의 정의

자신이 사용한 행동 + 상대방이 사용한 행동 + 거리 값(공격 가능 거리 / 공격 불가 거리)를 기반으로 합니다.

가능한 행동들 : 대기,공격,전진,후진,도약,속박,누크,방어 -> 8종류
거리 값 : 가깝다, 멀다 -> 2종류
누크,공격,속박의 사정거리는 동일하도록 세팅하였고, 따라서 공격 가능 거리 이내인지, 그렇지 않은지를 구분합니다. 따라서 2종류입니다.

상태의 값은 128종류(자신 8종류 * 상대 8종류 * 거리 2종류)입니다.


## 상태 전이 행렬
1. 무작위 값으로 채움
2. 이후 정책평가 및 개선을 통해 개선함


## 보상 시스템
적 사살시 보상, 사망시 처벌하는 기본 규칙을 가집니다.
적에게 치명 공격을 가하면 2점
적에게 데미지를 준다면 1점
자신이 데미지를 받는다면 -0.8점
자신이 치명 공격을 받는다면 -1.6점
이런 보상 시스템의 목적은 공격적으로 행동하기 위함입니다.

## 정책
"정책은 어떻게 호출하는가?"
정책[상태값]을 호출합니다.
상태 하나에 정책 하나가 연결되므로, 절대적 정책입니다.

정책을 완성할때는 정책 평가와 정책 개선이 반복적으로 실행되는데, 이를 기반으로 실제로 전투를 수행하지 않고, 간단히 "예측된 가치값"으로 모델을 최적화해나갑니다.
대략적인 수도코드는 다음과 같습니다.
### 수도코드
#### 준비된 값
정책[자신의 행동, 에너미 행동, 거리]
가치[자신의 행동, 에너미 행동, 거리]
#### 다음 루프 반복
##### 정책 평가
- 루프 A 시작 : For문을 돌아서 모든 상태값에 대한 정책을 하나씩 수정합니다. 그중에 한 루프에서의 한 상태값을 one이라고 합시다.
이때 편의를 위해 적의 행동은 one.Enemy, 자신의 행동을 one.My, 거리를 one.Distance라고 합시다.
- 다음 상태값을 예측합니다. 정책은 다음 행동을 리턴하도록 정의되어 있습니다 따라서 다음을 판단할 수 있습니다
다음 에너미 행동 = 정책[one.Enemy, one.My, one.Distance]
다음 자신 행동 = 정책[one.My, one.Enemy, one.Distance] 입니다
다음 거리는 one.Distance를 따르지만, 만약 one.Enemy나 one.My 중 하나가 전진하면 가까운 거리, one.Enemy나 one.My 중 하나가 후진하게 된다면 먼 거리가 됩니다.
이때 거리가 중요한 이유는 거리가 너무 멀면 공격할 수 없기 때문에, 가깝게 접근하도록 하기 위함입니다.

다음 에너미 행동 + 다음 플레이어 행동 + 다음 거리를 통해 미래 보상을 얻어냅니다.
가치[one.My, one.Enemy, one.Distance] = 보상 + 가치[다음 자신 행동, 다음 에너미 행동, 다음 거리]

##### 정책 개선
- 루프 A 시작 : For문을 돌아서 모든 상태값에 대한 정책을 하나씩 수정합니다. 그중에 한 루프에서의 한 상태값을 one이라고 합시다.
- 만약 정책[one]이 불가능하다면 정책 wait으로 바꿉니다.
- 루프 B 시작 : For문을 돌아서 정책[one]에서 가능한 행동 7가지를 돌아봅시다. 그중에 한 루프에서의 행동을 oneAction이라고 합시다.
- 만약 oneAction이 불가능하다면 continue합니다.
- 현재 정책인 정책[one]이 주는 가치보다, oneAction에서 주는 가치가 더 강하다면, 정책[one]을 oneAction으로 바꿉니다.
- 루프 B 종료
- 루프 A 종료
